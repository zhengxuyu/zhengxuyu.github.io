---
layout: blog-post
title: "A Survey of Meta Reinforcement Learning for Large Language Model training"
date: 28/07/2025 14:30:00 +0000
categories: PaperReading
tags: [Meta Reinforcement Learning, Large Language Models]
draft: false
tldr: "This blog is a paper reading summary on meta reinforcement learning, and its application in LLM training procedure."
bibliography: _bibliography/references.bib
---

## Key Takeaways

## Meta-RL Definition

<img src="/images/mrl/mrl.png" alt="Meta Reinforcement Learning Overview" width="400" style="max-width: 100%; height: auto; display: block; margin: 2rem auto;">

RL algorithms are traditionally designed, engineered, and tested by humans. The idea of meta-RL is instead to learn (parts of) an algorithm $f$ using machine learning. Where RL learns a policy, meta-RL learns the RL algorithm $f$ that outputs the policy. 

Formally, in standard RL, we learn a policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ that maps states to actions. In meta-RL, we learn a learning algorithm $f: \mathcal{M} \rightarrow \Pi$ that maps from a task distribution $\mathcal{M}$ to a policy space $\Pi$.

This does not remove all of the human effort from the process, but shifts it from directly designing and implementing the RL algorithms into developing the training environments and parameterizations required for learning parts
of them in a data-driven way {% cite beck2025tutorial %}.

The concept of meta-learning dates back to the early work of {% cite schmidhuber1987evolutionary %}, who introduced the idea of learning to learn. Later, {% cite thrun1998lifelong %} formalized lifelong learning algorithms, which laid the foundation for modern meta-RL approaches.

In recent years, the field of machine learning has witnessed remarkable advancements in large language models (LLMs) and their ability to perform in-context learning (ICL). However, a significant challenge remains: how to effectively scale and adapt these models during test time to improve their performance on specific tasks. This blog post explores innovative approaches recently that combines reinforcement learning with in-context learning to achieve dynamic test-time scaling {% cite wang2016learning %}.

## References

{% bibliography %} 